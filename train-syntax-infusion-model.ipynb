{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3b20c5c7-9a6c-4a7e-95b4-1b9253614950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scripts.applications.prediction_models.models.syntax_infused_model  import *\n",
    "from scripts.applications.prediction_models.models.sequential_model import *\n",
    "from scripts.applications.prediction_models.models.neural_cky import *\n",
    "from scripts.language_processing.language_builder.neural_builder.models.tn_pcfg import TNPCFG\n",
    "from scripts.language_processing.language_builder.neural_builder.models.utils import rebuild_T_from_head_left_right\n",
    "from scripts.applications.prediction_models.models.word_embedding import *\n",
    "\n",
    "def generate_random_eeg_data(cnt_channels = 19, cnt_time_samples = 1000): # unit: mV\n",
    "    return 0.1 * np.random.random_sample((cnt_channels, cnt_time_samples)) * 90\n",
    "\n",
    "def generate_random_eeg_microstates(cnt_channels = 19, cnt_time_samples = 1000): # unit: mV\n",
    "    return np.random.randint(0, cnt_words, (cnt_time_samples))\n",
    "\n",
    "def generate_random_segmented_eeg_data(cnt_channels = 19, cnt_time_samples = 1000): # unit: mV\n",
    "    dummy_eeg_data = generate_dummy_eeg_data(cnt_channels, cnt_time_samples)\n",
    "    split_points = [0]\n",
    "    p = np.random()\n",
    "    for i in range(1, cnt_time_samples):\n",
    "        if np.random() < p:\n",
    "            split_points.append(i)\n",
    "    split_points.append(cnt_time_samples)\n",
    "    return [dummy_eeg_data[split_points[i - 1], split_points[i]] for i in range(1, len(split_points))]\n",
    "\n",
    "def generate_random_corpus(word_count, cnt_article, article_max_length):\n",
    "    return [np.random.randint(0, word_count, size = (np.random.randint(0, article_max_length))) for _ in range(cnt_article)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e633522d-0a3b-44f5-b97b-1461b72a7872",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_configuration = {\n",
    "'cnt_channels': 19,\n",
    "'cnt_words': 10,\n",
    "'word_emb_size': 200,\n",
    "'syntax_infused_model_args': \n",
    "    {'NT': 20,\n",
    "              'T': 65,\n",
    "              's_dim': 50,\n",
    "              'r_dim': 75,\n",
    "              'word_emb_size': 200,\n",
    "            'cnt_words': 10,\n",
    "            'summary_parameters': True,\n",
    "    }\n",
    "}\n",
    "cnt_channels = main_configuration['cnt_channels']\n",
    "cnt_words = main_configuration['cnt_words']\n",
    "word_emb_size = main_configuration['word_emb_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "98368a8a-2cb8-46d8-b3f6-4f33ddf9a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "LNN_electrode_value_based_prediction_model = LNNElectrodeValueBasedPredictionModel(ncp_input_size = cnt_channels, hidden_size=100, output_size=1, sequence_length=1)\n",
    "combine_model = SimpleConcatCombing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b129c404-9163-4fe9-9795-f11435c9fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_model = WordEmbeddingModel(vocab_size = cnt_words, embedding_dim = word_emb_size, context_size = 2)\n",
    "word_embeddings = embeding_model.embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "041a78a5-c672-4065-bb68-eb46a88d4ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = main_configuration['syntax_infused_model_args']\n",
    "args['word_embeddings'] = word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7f96944b-d775-4d2e-9a5e-85a928956c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = generate_random_corpus(cnt_words, 1000, 2000)\n",
    "sentence_for_inference = corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "32eca0e3-ab59-4ef2-a5f2-8643159f779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build td pcfg\n",
      "device = cpu, NT = 20, T = 65, V = 10, s_dim = 50, r = 75, word_emb_size = 200\n",
      "begin forward>>>>>>>>>>>>>, input shape = torch.Size([1, 191])\n",
      "b, n = 1, 191\n",
      "torch.Size([65, 10])\n",
      "torch.Size([1, 191, 65]) torch.Size([1, 20]) torch.Size([1, 20, 75]) torch.Size([1, 85, 75]) torch.Size([1, 85, 75])\n"
     ]
    }
   ],
   "source": [
    "tn_pcfg = TNPCFG(args=args)\n",
    "inference = tn_pcfg.forward(torch.Tensor((np.array(sentence_for_inference)).reshape((1, len(corpus[0])))))\n",
    "# convert the inference result of 'unary' array to a more formal form.\n",
    "# the original unary is a 2-dimension array, in which i-th row is possibility of each terminate symbol directly deduct to the word at time point t.\n",
    "# in the origin ouput, unary[i] = unary[j] if word_sequence[i] = word_sequence[j]\n",
    "# now we put each unique word's feature into a 2-dimension matrix.\n",
    "inference_unary = np.zeros((args['T'], args['cnt_words']))\n",
    "original_unary = inference['unary'].detach().numpy()[0]\n",
    "sequence_length = original_unary.shape[0]\n",
    "for i in range(sequence_length):\n",
    "    inference_unary[:, sentence_for_inference[i]] = original_unary[i]\n",
    "\n",
    "def rebuild_T_from_head_left_right(head, left, right, NT, T):\n",
    "    r_dim = head.shape[1]\n",
    "    sum_NT_T = NT + T\n",
    "    T = np.zeros((NT * sum_NT_T * sum_NT_T))\n",
    "    for r in range(r_dim):\n",
    "        T += np.kron(np.kron(head[:, r].detach().numpy(), left[:, r].detach().numpy()), right[:, r].detach().numpy())\n",
    "    return T.reshape((NT, sum_NT_T, sum_NT_T))\n",
    "\n",
    "T = rebuild_T_from_head_left_right(inference['head'][0], inference['left'][0], inference['right'][0], args['NT'], args['T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "05025c29-ac78-4f52-9425-f2d60209e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "args['grammar_starts'] = inference['root'].detach().numpy()[0]\n",
    "args['grammar_preterminates'] = inference_unary\n",
    "args['grammar_double_nonterminates'] = T\n",
    "args['beam_search_strategy'] = select_tops\n",
    "args['preterminate_feature_generation_model'] = NN_CYK_FeatureCombingModel_Preterminate()\n",
    "args['merge_model'] = NN_CYK_FeatureCombingModel_NonPreterminate()\n",
    "nn_cyk_model = NN_CYK_Model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afddb65d-427c-4db1-b692-8c239e06b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_full_connection_prediction_model = FCPrediction(input_size = 228)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d4875a1d-f143-40ea-a3da-596266b2d8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "syntax_infused_model = \\\n",
    "    SyntaxInfusedModel(sequential_model = LNN_electrode_value_based_prediction_model\\\n",
    "                       ,syntax_model = nn_cyk_model\\\n",
    "                       , combining_model = combine_model\\\n",
    "                       , prediction_model = simple_full_connection_prediction_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed607ee8-4895-4321-9435-63802f4bed19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
