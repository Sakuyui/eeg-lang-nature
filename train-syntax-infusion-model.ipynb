{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b20c5c7-9a6c-4a7e-95b4-1b9253614950",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scripts.applications.prediction_models.models.syntax_infused_model  import *\n",
    "from scripts.applications.prediction_models.models.sequential_model import *\n",
    "from scripts.applications.prediction_models.models.neural_cky import *\n",
    "from scripts.language_processing.language_builder.neural_builder.models.tn_pcfg import TNPCFG\n",
    "from scripts.language_processing.language_builder.neural_builder.models.utils import rebuild_T_from_head_left_right\n",
    "from scripts.applications.prediction_models.models.word_embedding import *\n",
    "\n",
    "def generate_random_eeg_data(cnt_channels = 19, cnt_time_samples = 1000): # unit: mV\n",
    "    return 0.1 * np.random.random_sample((cnt_channels, cnt_time_samples)) * 90\n",
    "\n",
    "def generate_random_eeg_microstates(cnt_channels = 19, cnt_time_samples = 1000): # unit: mV\n",
    "    return np.random.randint(0, cnt_words, (cnt_time_samples))\n",
    "\n",
    "def generate_random_segmented_eeg_data(cnt_channels = 19, cnt_time_samples = 1000): # unit: mV\n",
    "    dummy_eeg_data = generate_dummy_eeg_data(cnt_channels, cnt_time_samples)\n",
    "    split_points = [0]\n",
    "    p = np.random()\n",
    "    for i in range(1, cnt_time_samples):\n",
    "        if np.random() < p:\n",
    "            split_points.append(i)\n",
    "    split_points.append(cnt_time_samples)\n",
    "    return [dummy_eeg_data[split_points[i - 1], split_points[i]] for i in range(1, len(split_points))]\n",
    "\n",
    "def generate_random_corpus(word_count, cnt_article, article_max_length):\n",
    "    return [np.random.randint(0, word_count, size = (np.random.randint(0, article_max_length))) for _ in range(cnt_article)]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e633522d-0a3b-44f5-b97b-1461b72a7872",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_configuration = {\n",
    "'cnt_channels': 19,\n",
    "'cnt_words': 10,\n",
    "'word_emb_size': 200,\n",
    "'syntax_infused_model_args': \n",
    "    {'NT': 20,\n",
    "              'T': 65,\n",
    "              's_dim': 50,\n",
    "              'r_dim': 75,\n",
    "              'word_emb_size': 200,\n",
    "            'cnt_words': 10,\n",
    "            'summary_parameters': True,\n",
    "    }\n",
    "}\n",
    "cnt_channels = main_configuration['cnt_channels']\n",
    "cnt_words = main_configuration['cnt_words']\n",
    "word_emb_size = main_configuration['word_emb_size']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98368a8a-2cb8-46d8-b3f6-4f33ddf9a0ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "LNN_electrode_value_based_prediction_model = LNNElectrodeValueBasedPredictionModel(ncp_input_size = cnt_channels, hidden_size=100, output_size=1, sequence_length=1)\n",
    "combine_model = SimpleConcatCombing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b129c404-9163-4fe9-9795-f11435c9fa30",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_model = WordEmbeddingModel(vocab_size = cnt_words, embedding_dim = word_emb_size, context_size = 2)\n",
    "word_embeddings = embeding_model.embeddings.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "041a78a5-c672-4065-bb68-eb46a88d4ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = main_configuration['syntax_infused_model_args']\n",
    "args['word_embeddings'] = word_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f96944b-d775-4d2e-9a5e-85a928956c9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = generate_random_corpus(cnt_words, 1000, 2000)\n",
    "sentence_for_inference = corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32eca0e3-ab59-4ef2-a5f2-8643159f779f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "build td pcfg\n",
      "device = cpu, NT = 20, T = 65, V = 10, s_dim = 50, r = 75, word_emb_size = 200\n",
      "begin forward>>>>>>>>>>>>>, input shape = torch.Size([1, 789])\n",
      "b, n = 1, 789\n",
      "torch.Size([65, 10])\n",
      "torch.Size([1, 789, 65]) torch.Size([1, 20]) torch.Size([1, 20, 75]) torch.Size([1, 85, 75]) torch.Size([1, 85, 75])\n"
     ]
    }
   ],
   "source": [
    "tn_pcfg = TNPCFG(args=args)\n",
    "inference = tn_pcfg.forward(torch.Tensor((np.array(sentence_for_inference)).reshape((1, len(corpus[0])))))\n",
    "# convert the inference result of 'unary' array to a more formal form.\n",
    "# the original unary is a 2-dimension array, in which i-th row is possibility of each terminate symbol directly deduct to the word at time point t.\n",
    "# in the origin ouput, unary[i] = unary[j] if word_sequence[i] = word_sequence[j]\n",
    "# now we put each unique word's feature into a 2-dimension matrix.\n",
    "inference_unary = np.zeros((args['T'], args['cnt_words']))\n",
    "original_unary = inference['unary'].detach().numpy()[0]\n",
    "sequence_length = original_unary.shape[0]\n",
    "for i in range(sequence_length):\n",
    "    inference_unary[:, sentence_for_inference[i]] = original_unary[i]\n",
    "\n",
    "def rebuild_T_from_head_left_right(head, left, right, NT, T):\n",
    "    r_dim = head.shape[1]\n",
    "    sum_NT_T = NT + T\n",
    "    T = np.zeros((NT * sum_NT_T * sum_NT_T))\n",
    "    for r in range(r_dim):\n",
    "        T += np.kron(np.kron(head[:, r].detach().numpy(), left[:, r].detach().numpy()), right[:, r].detach().numpy())\n",
    "    return T.reshape((NT, sum_NT_T, sum_NT_T))\n",
    "\n",
    "T = rebuild_T_from_head_left_right(inference['head'][0], inference['left'][0], inference['right'][0], args['NT'], args['T'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "05025c29-ac78-4f52-9425-f2d60209e021",
   "metadata": {},
   "outputs": [],
   "source": [
    "args['grammar_starts'] = inference['root'].detach().numpy()[0]\n",
    "args['grammar_preterminates'] = inference_unary\n",
    "args['grammar_double_nonterminates'] = T\n",
    "args['beam_search_strategy'] = select_tops\n",
    "args['preterminate_feature_generation_model'] = NN_CYK_FeatureCombingModel_Preterminate()\n",
    "args['merge_model'] = NN_CYK_FeatureCombingModel_NonPreterminate()\n",
    "nn_cyk_model = NN_CYK_Model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "afddb65d-427c-4db1-b692-8c239e06b151",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_full_connection_prediction_model = FCPrediction(input_size = 228)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4875a1d-f143-40ea-a3da-596266b2d8ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fix base sequential prediction model...\n"
     ]
    }
   ],
   "source": [
    "syntax_infused_model = \\\n",
    "    SyntaxInfusedModel(sequential_model = LNN_electrode_value_based_prediction_model\\\n",
    "                       ,syntax_model = nn_cyk_model\\\n",
    "                       , combining_model = combine_model\\\n",
    "                       , prediction_model = simple_full_connection_prediction_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed607ee8-4895-4321-9435-63802f4bed19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.applications.prediction_models.models.train.train import ModelTrainer\n",
    "\n",
    "trainer = ModelTrainer({\n",
    "    'model': syntax_infused_model,\n",
    "    'optimizer': optim.Adam(syntax_infused_model.parameters(), lr = 0.001),\n",
    "    'train_arg':{\n",
    "        'shuffle_trainning_set': True,\n",
    "        'train_only': True,\n",
    "        'batch_size_train': 1,\n",
    "        'is_unsupervisor_learning': False,\n",
    "        'clip_gradient': False,\n",
    "        'clip': 0,\n",
    "        'is_print_loss_per_batch': False,\n",
    "        'max_epoches': 10\n",
    "    },\n",
    "    'save_model_automatically': False\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dd299977-d5cf-41c6-bb15-8d9c96ba81d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_eeg_data = generate_random_eeg_data(cnt_channels = cnt_channels).T\n",
    "random_labels = np.random.randint(0, 2, random_eeg_data.shape[1])\n",
    "random_words = np.random.randint(0, cnt_words, size=random_eeg_data.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ce9d81ba-8a74-4521-8a32-85c7e61e092e",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "not a sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[21], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_eeg_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_words\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_labels\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Desktop\\MainResearchProjects\\eeg-language\\scripts\\applications\\prediction_models\\models\\train\\train.py:79\u001b[0m, in \u001b[0;36mModelTrainer.train\u001b[1;34m(self, train_dataset, test_dataset, time_sequence)\u001b[0m\n\u001b[0;32m     77\u001b[0m train_only \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_arg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain_only\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     78\u001b[0m batch_size_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_arg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size_train\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m---> 79\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m DataLoader(TensorDataset(\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mFloatTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43mINDEX_DATASET_INPUTS\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mFloatTensor(train_dataset[INDEX_DATASET_TARGETS])),\\\n\u001b[0;32m     80\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size_train, shuffle\u001b[38;5;241m=\u001b[39mshuffle)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m train_only:\n\u001b[0;32m     82\u001b[0m     batch_size_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_arg[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size_test\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mTypeError\u001b[0m: not a sequence"
     ]
    }
   ],
   "source": [
    "trainer.train([{'eeg_data':random_eeg_data, 'words':random_words}], [torch.Tensor(random_labels)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9bdb6b58-e0d8-47d4-a95b-b4a7a942331c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sd = torch.utils.data.DataLoader(torch.utils.data.StackDataset([{'eeg_data':random_eeg_data, 'words':random_words}, {'eeg_data':random_eeg_data, 'words':random_words}]), batch_size  = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a7f84b67-d744-4c4f-83e5-c9ef17df4f81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "for s in sd:\n",
    "    print(len(s[0]['words']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61691cb5-c7a7-497d-830c-32d081459885",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
